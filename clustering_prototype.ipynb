{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQljMrW6zX0s"
   },
   "source": [
    "# Clustering Prototype - Initial Version\n",
    "## Sentence embedding, dimensionality reduction, feature selection, clustering, and explaining clusters.\n",
    "\n",
    "---\n",
    "\n",
    "This file will allow you to set variables to run clustering and explainability on a dataset. In the next cell, you will be able to:\n",
    "\n",
    "1.   Specify a dataset. The dataset must be in .csv format and must have at least one textual column to perform clustering on. If you wish to perform feature selection, the dataset must also have a categorical label column. The following datasets are already supported by this repo, but you can also upload your own dataset (instructions provided in next cell):\n",
    "     - [IMDB Movie Reviews](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
    "     - [Customer Support Ticket Dataset](https://www.kaggle.com/datasets/suraj520/customer-support-ticket-dataset)\n",
    "     - Synthetic Customer Data (synthetic conversations with customer support chatbot)\n",
    "2.   Specify an embedding method. This file currently supports:\n",
    "     - Sentence-BERT ([paper](https://arxiv.org/abs/1908.10084), [documentation](https://www.sbert.net/))\n",
    "     - LLM2Vec ([paper](https://arxiv.org/abs/2404.05961), [GitHub](https://github.com/McGill-NLP/llm2vec))\n",
    "2.   Flag if you would like dimensionality reduction (PCA) to occur\n",
    "3.   Flag if you would like feature selection (decision trees) to occur\n",
    "\n",
    "Based on these options, the file will run K-means clustering and labeling on the dataset.\n",
    "\n",
    "Finally, it will print several data points close to each cluster centroid to help explain the cluster groupings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATLd6Ei23DsK"
   },
   "source": [
    "## Set options for clustering\n",
    "\n",
    "For each of the sections in the following cell, enable the option you wish to use (by uncommenting it) and ensure the other values are commented out. For any of the options, you can proceed with default values and later come back to modify them.\n",
    "\n",
    "- For dataset selection:\n",
    "  - Set the `dataset_sample_ratio` (the percentage of data the model will actually look at); for very large datasets, a fraction (few thousand rows) is a good benchmark\n",
    "  - Uncomment the `dataset` you would like to use, and specify a `text_col` (the column name to perform clustering on)\n",
    "  - If you wish to do feature selection (decision trees), specify a `label_col` as well (the column name to make predictions on)\n",
    "  - Optionally: provide a custom dataset by uploading a `.csv` file in this repository, and set the `dataset`, `text_col` and optionally the `label_col` variables as described above\n",
    "\n",
    "- For embedding methods, the options are:\n",
    "  - Sentence BERT\n",
    "  - LLM2Vec (note that this option may not work well on your machine due to it requiring large amounts of device memory)\n",
    "\n",
    "- For dimensionality reduction, you can:\n",
    "  - Enable `pca` and provide a number of dimensions for PCA\n",
    "  - Or, disable `pca` by commenting it out\n",
    "\n",
    "- For feature selection, you can:\n",
    "  - Enable `decision_trees` and provide a `feature_importance_threshold`  (minimum importance threshold for selection) as well as a `test_train_ratio` (for training decision trees); feel free to proceed with our default values for these\n",
    "  - Or, disable `decision_trees` by commenting it out\n",
    "\n",
    "- For cluster count, you can specify:\n",
    "  - A method to determine number of clusters for k-means (WCSS Elbow or Silhouette Score)\n",
    "\n",
    "- For printing data points from each cluster centroid at the end, you can specify:\n",
    "  - The number of points you would like to display from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sAJedakN2ZXn"
   },
   "outputs": [],
   "source": [
    "# Dataset selection -----------------------------------\n",
    "dataset_sample_ratio = 0.05 # percentage of dataset to actually look at\n",
    "\n",
    "# --- IMDB movie reviews // 50k observations ---\n",
    "dataset = \"IMDB.csv\"\n",
    "text_col = \"review\"\n",
    "label_col = \"sentiment\"\n",
    "\n",
    "# --- Customer support tickets // ~600 observations ---\n",
    "# dataset = \"helpdesk_customer_tickets.csv\"\n",
    "# text_col = \"answer\"\n",
    "# label_col = \"priority\"\n",
    "\n",
    "# --- Synthetic customer support conversation with chatbot // ~500 observations ---\n",
    "# dataset = \"Synthetic_Customer_Data.csv\"\n",
    "# text_col = \"Conversation\"\n",
    "\n",
    "\n",
    "# Embedding method -----------------------------------\n",
    "method = \"s_bert\"\n",
    "# method = \"llm2vec\"\n",
    "\n",
    "# Dimensionality reduction (PCA) ---------------------\n",
    "# pca = False\n",
    "pca = True\n",
    "pca_dimensions = 10\n",
    "\n",
    "# Feature selection (decision trees)  ----------------\n",
    "# decision_trees = False\n",
    "decision_trees = True\n",
    "feature_importance_threshold  = 0.025  # minimum importance threshold for selection\n",
    "test_train_ratio = 0.3  # ratio for training decision trees\n",
    "\n",
    "# Method to determine number of clusters for k-means --\n",
    "num_clusters_method = \"WCSS Elbow\"\n",
    "# num_clusters_method = \"Silhouette Score\"\n",
    "\n",
    "# Num example data points to print per cluster ---------\n",
    "example_count = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXSvOxxy4fGr"
   },
   "source": [
    "## Imports and Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will pip install all necessary packages for this file\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jp7j_bG7zX0w",
    "outputId": "728a7ac4-f599-4ea5-cab9-271306f94b37"
   },
   "outputs": [],
   "source": [
    "# This cell imports all necessary libraries for this file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "!env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True  # allow torch to allocate memory more effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RE8IqATgahv"
   },
   "source": [
    "### LLM2Vec additional requirement: Request model access\n",
    "\n",
    "If you plan to use LLM2Vec, you need to create a HuggingFace account and [access token](https://huggingface.co/settings/tokens) (a read-level token should suffice). Then, request access to to the model [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2), which is currently under gated repo access, by clicking the button on the linked webpage. The request should be approved within seconds.\n",
    "\n",
    "Run the following cell, and when prompted, paste in your access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMfFKOrNf6U0"
   },
   "outputs": [],
   "source": [
    "if method == \"llm2vec\":\n",
    "  !pip install llm2vec -q\n",
    "  !huggingface-cli login\n",
    "\n",
    "  from llm2vec import LLM2Vec\n",
    "  import torch\n",
    "  from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "  from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering pipeline\n",
    "\n",
    "Now that all the options are set, you can simply run all following cells. After the PCA and K-means graphs are generated, you can examine them to optionally manually set a number of dimensions for PCA, and/or manually set the number of clusters for K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eKhn6hNN4nsj"
   },
   "outputs": [],
   "source": [
    "# Read dataset --------------------------------------------------------------\n",
    "df = pd.read_csv(dataset)\n",
    "df = df.sample(frac=dataset_sample_ratio)\n",
    "text = df[text_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sx3lshCYbv3N",
    "outputId": "9fb7f5f4-685f-4b96-dd4f-be22b9964824"
   },
   "outputs": [],
   "source": [
    "# Calculate embeddings ------------------------------------------------------\n",
    "# Note: this can be a time-consuming step depending on the dataset's size\n",
    "\n",
    "if method == \"s_bert\":\n",
    "  embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "  embeddings = embedding_model.encode(text.tolist(), convert_to_tensor=False)\n",
    "\n",
    "elif method == \"llm2vec\":\n",
    "  # Following code adapted from https://huggingface.co/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\n",
    "      \"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp\"\n",
    "  )\n",
    "  config = AutoConfig.from_pretrained(\n",
    "      \"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp\", trust_remote_code=True\n",
    "  )\n",
    "  model = AutoModel.from_pretrained(\n",
    "      \"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp\",\n",
    "      trust_remote_code=True,\n",
    "      config=config,\n",
    "      torch_dtype=torch.bfloat16,\n",
    "      device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "  )\n",
    "  model = PeftModel.from_pretrained(\n",
    "      model,\n",
    "      \"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp\",\n",
    "  )\n",
    "  model = model.merge_and_unload()  # This can take several minutes on CPU\n",
    "\n",
    "  # Loading supervised model. This loads the trained LoRA weights on top of MNTP model. Hence the final weights are -- Base model + MNTP (LoRA) + supervised (LoRA).\n",
    "  model = PeftModel.from_pretrained(\n",
    "      model, \"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised\"\n",
    "  )\n",
    "\n",
    "  # Wrapper for encoding and pooling operations\n",
    "  l2v = LLM2Vec(model, tokenizer, pooling_mode=\"mean\", max_length=512)\n",
    "\n",
    "  embeddings = l2v.encode(text.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "If PCA is enabled, we will display a scree plot to examine how much of the variance can be explained by each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wm-yNKJKEO3_"
   },
   "outputs": [],
   "source": [
    "# Dimensionality Reduction (PCA) --------------------------------------------\n",
    "\n",
    "# Create scree plot of possible number of components / dimensions\n",
    "if pca:\n",
    "  PCA_MAX_DIMENSION = 21\n",
    "  n_components = range(2, PCA_MAX_DIMENSION) # range of testing\n",
    "\n",
    "  # Perform PCA for each number of components and store explained variance\n",
    "  explained_variance_ratios = []\n",
    "  cumulative_variance_ratios = []\n",
    "\n",
    "  for n in n_components:\n",
    "      pca = PCA(n_components=n)\n",
    "      pca.fit(embeddings)\n",
    "      explained_variance_ratios.append(pca.explained_variance_ratio_)\n",
    "      cumulative_variance_ratios.append(np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "  # Average explained variance ratio over the number of components\n",
    "  avg_explained_variance_ratio = [np.mean(ratio) for ratio in explained_variance_ratios]\n",
    "\n",
    "  # Plot the scree plot\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.bar(n_components, avg_explained_variance_ratio, alpha=0.7, label='Avg Explained Variance')\n",
    "  plt.plot(n_components, [cum[-1] for cum in cumulative_variance_ratios], label='Cumulative Variance', color='red', marker='o')\n",
    "\n",
    "  # Add titles and labels\n",
    "  plt.title('Scree Plot for Components 2 to 20')\n",
    "  plt.xlabel('Number of Principal Components')\n",
    "  plt.ylabel('Explained Variance Ratio')\n",
    "  plt.xticks(n_components)\n",
    "  plt.legend(loc='best')\n",
    "  plt.grid(alpha=0.3)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Optionally, you can examine this plot and manually reset the `pca_dimension` number in the next cell. The optimal number of dimensions is the \"elbow point\" of the graph, where additional components contribute less to explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: In the next line, manually enter a new number of pca_dimensions to use based on the plot\n",
    "# Else, we default to the number set at the beginning of the file\n",
    "pca_dimensions = pca_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "if pca:\n",
    "  pca_model = PCA(n_components=pca_dimensions)\n",
    "  embeddings = pca_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlbm-B23IqrV"
   },
   "source": [
    "### Incorporate feature selection using decision trees\n",
    "\n",
    "Recall that feature selection requires your dataset to have a `label_col`, which you can set at the very beginning. Feature selection will \"shrink\" or drop the non-important features with respect to the output class (or `label_col`), and \"expand\" the features that are more important to determining this output class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnU_M7U9GxWC"
   },
   "outputs": [],
   "source": [
    "# Feature Selection (Decision Trees) ----------------------------------------\n",
    "if decision_trees:\n",
    "  X_train, X_test, y_train, y_test = train_test_split(embeddings, df[label_col], random_state = 8, test_size = test_train_ratio)\n",
    "\n",
    "  # Fit the decision tree model\n",
    "  decision_tree_model = DecisionTreeClassifier()\n",
    "  decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "  # Access feature importances\n",
    "  feature_importances = decision_tree_model.feature_importances_\n",
    "\n",
    "  # Delete low-importance features\n",
    "  low_importance_indices = np.where(feature_importances < feature_importance_threshold)[0] # random sampling prob. -- 1/384 = 0.0026\n",
    "  embeddings_small = np.delete(embeddings, low_importance_indices, axis=1)\n",
    "  X_train_reduced = np.delete(X_train, low_importance_indices, axis=1)\n",
    "  X_test_reduced = np.delete(X_test, low_importance_indices, axis=1)\n",
    "  print(f\"Embeddings went from size {len(X_train[0])} to {len(X_train_reduced[0])}\")\n",
    "  \n",
    "else:\n",
    "  embeddings_small = embeddings  # regardless of whether feature selection is performed, the next section uses embeddings_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Xih5Bg6JZ6e"
   },
   "source": [
    "### K-means clustering\n",
    "\n",
    "First, we will calculate and plot WCCS (within-cluster sum of squares) and silhouette scores for a range of `k` values, where `k` is the number of clusters used to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "ZfPRMn_4Jc3F",
    "outputId": "958f17a4-c85a-41a4-e503-16ebe46f99f0"
   },
   "outputs": [],
   "source": [
    "wcss_list = []  # within-cluster sum of squares\n",
    "silhouette_scores_list = []\n",
    "\n",
    "# Calculate WCSS and silhouette scores for each k\n",
    "upper_bound_k = 50\n",
    "for k in range(2, min(upper_bound_k, embeddings_small.shape[0])):\n",
    "    # WCSS\n",
    "    kmeans_experiment = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans_experiment.fit(embeddings_small)\n",
    "    wcss_list.append(kmeans_experiment.inertia_)\n",
    "\n",
    "    # silhouette scores\n",
    "    labels = kmeans_experiment.fit_predict(embeddings_small)\n",
    "    score = silhouette_score(embeddings_small, labels)\n",
    "    silhouette_scores_list.append(score)\n",
    "\n",
    "# Plot WCSS values\n",
    "plt.plot(range(2, min(upper_bound_k, embeddings_small.shape[0])), wcss_list, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.plot(range(2, min(upper_bound_k, embeddings_small.shape[0])), silhouette_scores_list, marker='o')\n",
    "plt.title('Silhouette Score vs. Number of Clusters (K)')\n",
    "plt.xlabel('Number of clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZWiync3a-Hg"
   },
   "source": [
    "Based on the method to choose `k` selected at the beginning, the following cell will automatically calculate an appropriate `k` to use to run k-means clustering on the dataset.\n",
    "\n",
    "Optionally: you can examine the graphs and manually choose the optimal value for `k` at the top of the next cell. This would be the \"elbow point\" of the WCSS graph, or the value that maximizes the silhouette score in the silhouette score graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: In the next line, manually enter a positive value for k to use based on the plots\n",
    "# If you would like the code to automatically find a value for k, leave num_clusters as None\n",
    "num_clusters = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuDYb2v9a-Hh",
    "outputId": "14c62878-0b23-4e84-9c69-b2f09cdf4ca3"
   },
   "outputs": [],
   "source": [
    "# Automatically find a value for k based on the method specified earlier\n",
    "if num_clusters == None:\n",
    "    \n",
    "    if num_clusters_method == \"Silhouette Score\":\n",
    "        max_index = np.argmax(silhouette_scores_list)\n",
    "        num_clusters = max_index + 2  # index 0 corresponds to 2 clusters\n",
    "        \n",
    "    elif num_clusters_method == \"WCSS Elbow\":  # elbow point occurs as the first point whose slope with the previous point is less than 1\n",
    "        i = 1\n",
    "        while i < len(wcss_list) and abs(wcss_list[i-1]-wcss_list[i]) > 1:\n",
    "            i += 1\n",
    "        num_clusters = i # index 0 corresponds to neighbor_range + 1 clusters\n",
    "\n",
    "    else:\n",
    "        print(\"Error: Must choose an positive integer number of clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering\n",
    "print(f\"Chosen number of clusters: {num_clusters}\\n\")\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(embeddings_small)\n",
    "\n",
    "# Print WCCS (within-cluster sum of squares) metric\n",
    "wcss = kmeans.inertia_\n",
    "print(\"Within-Cluster Sum of Squares:\", wcss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbm667AsM6_r"
   },
   "source": [
    "## Find closest examples to cluster centroids\n",
    "\n",
    "Currently, we are using cosine similarity to find the data point closest to the cluster centroids as a proxy for explaining or representing that cluster. We can also explore other ways (e.g. using LLMs) to explain clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OhEiFZGRzX06",
    "outputId": "f45353da-dfbf-4515-b5d8-c2e465e8da13"
   },
   "outputs": [],
   "source": [
    "# For each cluster centroid, print the closest data point\n",
    "corpus = list(df[text_col])\n",
    "centroid_embeddings = kmeans.cluster_centers_\n",
    "for i, centroid_vec in enumerate(centroid_embeddings):\n",
    "    similarity_scores = util.cos_sim(centroid_vec, embeddings_small)\n",
    "    closest_data_point = corpus[int(similarity_scores.argmax())]\n",
    "    print(f\"Centroid of cluster {i}:\\n-----\\n{closest_data_point}\\n-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gEyDHnjvUoa_",
    "outputId": "f79d87ee-4d87-4d1b-a0b3-9bd2aa96b73e"
   },
   "outputs": [],
   "source": [
    "# Print a few data points from each cluster to also help with explainability\n",
    "for i in range(num_clusters):\n",
    "    print(f\"\\n\\n\\nCluster {i}:\")\n",
    "    samples = df[df['cluster'] == i][text_col].tolist()[:example_count]\n",
    "    for s in samples:\n",
    "        print(\"--\")\n",
    "        print(s)\n",
    "    print(\"--\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
